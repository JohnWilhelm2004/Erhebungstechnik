results[results$algorithm == algo_names, "Absolute_Error"] =
error
results[results$algorithm == algo_names, "Runtime"] =
benchmark_res$time[i]
}
return(results)
}
# testVarianceFunction
# Prüft, ob der Algorithmus für Standard-Fälle das gleiche Ergebnis liefert
# wie die Referenzimplementierung var().
testVarianceFunction <- function(varianceAlgo) {
singleTest <- function(vec) {
res <- varianceAlgo(vec)
# Wir nutzen expect_equal mit Toleranz wegen Gleitkommazahlen
expect_equal(res, var(vec), tolerance = 1e-8)
}
test_that("Random Vectors", {
set.seed(1273)
for (n in c(10, 100, 1000)) {
singleTest(rnorm(n))
singleTest(runif(n) * 100)
}
})
test_that("Special Vectors", {
singleTest(1:10)
singleTest(rep(1, 10)) # Varianz sollte 0 sein
singleTest(seq(0, 1, length.out = 100))
})
}
# Ausführen der Tests
# Hinweis: textbookVar könnte bei extremen Werten hier schon scheitern,
# wir testen hier aber zunächst "normale" Bereiche.
testVarianceFunction(twoPassVar)
testVarianceFunction(youngsCramerVar)
testVarianceFunction(baseVar)
testVarianceFunction(textbookVar)
print("Alle Basis-Tests bestanden.")
test_that("run_experiment liefert korrekte Struktur zurück", {
k_test <- 1
n_test <- 50
# Funktion ausführen
res <- run_experiment(k = k_test, n = n_test)
# 1. Test auf Klasse
expect_s3_class(res, "data.frame")
# 2. Test auf Dimensionen (4 Algorithmen -> 4 Zeilen, 5 Spalten)
# Spalten: k, n, algorithm, Absolute_Error, Runtime
expect_equal(nrow(res), 4)
expect_equal(ncol(res), 5)
# 3. Test auf Spaltennamen
expected_cols <- c("k", "n", "algorithm", "Absolute_Error", "Runtime")
expect_named(res, expected_cols, ignore.case = TRUE)
# 4. Test auf Inhalt (Datentypen und Wertebereiche)
expect_type(res$k, "double")
expect_type(res$algorithm, "character") # Oder "integer" falls factor
expect_true(all(res$k == k_test))       # k muss übereinstimmen
expect_true(all(res$n == n_test))       # n muss übereinstimmen
# Laufzeit und Fehler müssen positiv oder 0 sein (numeric)
expect_true(all(res$Absolute_Error >= 0))
expect_true(all(res$Runtime > 0))
})
test_that("run_experiment funktioniert robust für verschiedene k und n", {
# 1. Definieren verschiedener Szenarien
# Wir testen kleine n, große n und verschiedene k-Werte
test_scenarios <- list(
list(k = 1,  n = 10),    # Minimales Setup
list(k = 5,  n = 100),   # Standardfall
list(k = 12, n = 1000)   # Hohe Verschiebung (Testet numerische Stabilität der Rückgabe)
)
for (setup in test_scenarios) {
# Ausführen
res <- run_experiment(k = setup$k, n = setup$n)
# --- Strukturtests ---
# Prüfen, ob Dimensionen stimmen
expect_equal(nrow(res), 4,
info = paste("Fehler bei k=", setup$k, "n=", setup$n))
# --- Inhaltliche Konsistenztests ---
# Prüfen, ob die Parameter korrekt durchgereicht wurden
expect_true(all(res$k == setup$k),
label = paste("Spalte 'k' fehlerhaft bei Input k=", setup$k))
expect_true(all(res$n == setup$n),
label = paste("Spalte 'n' fehlerhaft bei Input n=", setup$n))
# --- Plausibilitätstests ---
# Laufzeit muss positiv sein
expect_true(all(res$Runtime > 0))
# Fehler darf nicht NA sein (auch wenn er bei k=12 für textbookVar groß sein wird)
expect_false(any(is.na(res$Absolute_Error)),
label = paste("NAs gefunden bei k=", setup$k))
}
})
# Testfunktion
test_that("Strukturtests für Experiment und Studie", {
# Test 1: run_experiment muss 4 Zeilen liefern (1 pro Algorithmus)
exp_res <- run_experiment(k = 1, n = 10)
expect_s3_class(exp_res, "data.frame")
expect_equal(nrow(exp_res), 4)
expect_true(all(exp_res$Runtime > 0)) # Laufzeit muss positiv sein
# Test 2: run_study muss (Anzahl k) * (Wiederholungen) * 4 Zeilen liefern
# Wir testen kurz mit 2 k-Werten und 2 Wiederholungen -> 2*2*4 = 16 Zeilen
study_res <- run_study(k_values = 1:2, repititions = 2)
expect_equal(nrow(study_res), 16)
expect_named(study_res, c("k", "n", "algorithm", "Absolute_Error", "Runtime"))
})
library(testthat)
library(microbenchmark)
library(ggplot2)
source("var.R")
run_experiment = function(k, n = 100) {
sample = sample(1:n) + 10^k
trueVar = (n^2 - 1)/12
var_algorithms = list(
textbook = textbookVar,
twoPass = twoPassVar,
youngsCramer = youngsCramerVar,
baseR = baseVar
)
results = data.frame("k" = k,
"n" = n,
"algorithm" = names(var_algorithms)
)
# Laufzeit berechnen
benchmark_res = microbenchmark(textbookVar(sample),
twoPassVar(sample),
youngsCramerVar(sample),
baseVar(sample),
times = 1,
warmup = 1
)
# Fehler berechnen und Ergebnisse zusammenführen
for (i in 1:length(var_algorithms)) {
algo = var_algorithms[[i]]
algo_names = names(var_algorithms)[i]
error = abs(trueVar - algo(sample))
results[results$algorithm == algo_names, "Absolute_Error"] =
error
results[results$algorithm == algo_names, "Runtime"] =
benchmark_res$time[i]
}
return(results)
}
run_study = function(k_values = 1:16, repititions = 10) {
res = list()
counter = 1
for (k in k_values) {
for (r in 1:repititions) {
res[[counter]] = run_experiment(k)
counter = counter + 1
}
}
final_res = do.call(rbind, res)
return(final_res)
}
# Tests
test_that("Strukturtests für Experiment und Studie", {
# Test 1: run_experiment muss 4 Zeilen liefern (1 pro Algorithmus)
exp_res <- run_experiment(k = 1, n = 10)
expect_s3_class(exp_res, "data.frame")
expect_equal(nrow(exp_res), 4)
expect_true(all(exp_res$Runtime > 0)) # Laufzeit muss positiv sein
# Test 2: run_study muss (Anzahl k) * (Wiederholungen) * 4 Zeilen liefern
# Wir testen kurz mit 2 k-Werten und 2 Wiederholungen -> 2*2*4 = 16 Zeilen
study_res <- run_study(k_values = 1:2, repititions = 2)
expect_equal(nrow(study_res), 16)
expect_named(study_res, c("k", "n", "algorithm", "Absolute_Error", "Runtime"))
})
set.seed(123)
results = run_study()
# Visualisierung der Fehler
error_plot = ggplot(
results, aes(x = factor(k), y = Absolute_Error, col = algorithm)) +
geom_boxplot(position = position_dodge(width = 0.75),
fill = NA) +
scale_y_log10(labels = scales::label_scientific()) +
labs(
title = "Absoluter Fehler in Abhängigkeit des
Schwierigkeitsgrads k",
x = "k (Exponent der Verschiebung 10^k)",
y = "Absoluter Fehler (log-Skala)",
color = "Algorithmus"
) +
theme_minimal()
error_plot
runtime_plot = ggplot(
sim_data, aes(x = factor(k), y = Time, fill = Algorithm)) +
geom_boxplot(outlier.size = 0.5, lwd = 0.3) +
scale_y_log10() +
labs(
title = "Laufzeitvergleich der Varianz-Algorithmen",
subtitle = "Laufzeit in Abhängigkeit von k (logarithmische Skala)",
x = "k (Verschiebung 10^k)",
y = "Laufzeit [Nanosekunden]",
fill = "Algorithmus"
) +
theme_bw() +
runtime_plot
runtime_plot = ggplot(
results, aes(x = factor(k), y = Time, fill = Algorithm)) +
geom_boxplot(outlier.size = 0.5, lwd = 0.3) +
scale_y_log10() +
labs(
title = "Laufzeitvergleich der Varianz-Algorithmen",
subtitle = "Laufzeit in Abhängigkeit von k (logarithmische Skala)",
x = "k (Verschiebung 10^k)",
y = "Laufzeit [Nanosekunden]",
fill = "Algorithmus"
) +
theme_bw() +
runtime_plot
runtime_plot = ggplot(
results, aes(x = factor(k), y = Time, fill = Algorithm)) +
geom_boxplot(outlier.size = 0.5, lwd = 0.3) +
scale_y_log10() +
labs(
title = "Laufzeitvergleich der Varianz-Algorithmen",
subtitle = "Laufzeit in Abhängigkeit von k (logarithmische Skala)",
x = "k (Verschiebung 10^k)",
y = "Laufzeit [Nanosekunden]",
fill = "Algorithmus"
) +
theme_bw()
runtime_plot
runtime_plot = ggplot(
results, aes(x = factor(k), y = Runtime, fill = algorithm)) +
geom_boxplot(outlier.size = 0.5, lwd = 0.3) +
scale_y_log10() +
labs(
title = "Laufzeitvergleich der Varianz-Algorithmen",
subtitle = "Laufzeit in Abhängigkeit von k (logarithmische Skala)",
x = "k (Verschiebung 10^k)",
y = "Laufzeit [Nanosekunden]",
fill = "Algorithmus"
) +
theme_bw()
runtime_plot
install.packages("psych")
data(bfi)
bfi
data("bfi")
data(bfi)
library(psych)
data(bfi)
bfi
bfi.dictionary
bfi
bfi$A1
?bfi
bfi[, c("O1", "O2", "O3", "O4", "05")]
bfi
bfi[, O1]
bfi[, "O1"]
bfi[, c("O1", "O2", "O3", "O4", "O5")]
cov(bfi[, c("O1", "O2", "O3", "O4", "O5")])
?cov
cov(bfi[, c("O1", "O2", "O3", "O4", "O5")], na.rm = TRUE)
cov(bfi[, c("O1", "O2", "O3", "O4", "O5")], use = pairwise.complete.obs)
cov(bfi[, c("O1", "O2", "O3", "O4", "O5")], use = "pairwise.complete.obs")
bfi[, c("O1", "O2", "O3", "O4", "O5")]
Var(sum(bfi[, c("O1", "O2", "O3", "O4", "O5")]))
var(sum(bfi[, c("O1", "O2", "O3", "O4", "O5")]))
?var
var(sum(bfi[, c("O1", "O2", "O3", "O4", "O5")]), use = "pairwise.complete.obs")
var(sum(bfi[, c("O1", "O2", "O3", "O4", "O5")]), use = "pairwise.complete.obs")
?var
cov(bfi[, c("O1", "O2", "O3", "O4", "O5")], use = "complete.obs")
var(sum(bfi[, c("O1", "O2", "O3", "O4", "O5")]), use = "complete.obs")
cov(bfi[, c("O1", "O2", "O3", "O4", "O5")], use = "na.or.complete")
var(sum(bfi[, c("O1", "O2", "O3", "O4", "O5")]), use = "na.or.complete")
var(sum(bfi[, c("O1", "O2", "O3", "O4", "O5")]), use = "all.obs")
var(sum(bfi[, c("O1", "O2", "O3", "O4", "O5")]), na.rm = TRUE)
var(rowSums(bfi[, c("O1", "O2", "O3", "O4", "O5")]), na.rm = TRUE)
covariance_mat = cov(bfi[, c("O1", "O2", "O3", "O4", "O5")], use = "na.or.complete")
covariance_mat
variance = var(rowSums(bfi[, c("O1", "O2", "O3", "O4", "O5")]), na.rm = TRUE)
variance
View(bfi)
variance
rowSums(covariance_mat)
variance
sum(covariance_mat)
sum(covariance_mat) == variance
?psych
?psych:alpha
?`psych-package`
openness = bfi[, c("O1", "O2", "O3", "O4", "O5")]
covariance_mat = cov(openness, use = "na.or.complete")
covariance_mat
variance = var(rowSums(openness), na.rm = TRUE)
variance
sum(covariance_mat) == variance
alpha(openness, check.keys = TRUE)
bfi
?bfi
recode = function(x) {
for (i in x) {
res = 6 - x[i]
x[i] = res
}
}
openness = bfi[, c("O1","O2", "O3", "O4", "O5")]
openness$O2 = recode(openness$O2)
openness$O5 = recode(openness$O5)
covariance_mat = cov(openness, use = "na.or.complete")
covariance_mat
variance = var(rowSums(openness), na.rm = TRUE)
variance
sum(covariance_mat) == variance
alpha(openness, check.keys = TRUE)
variance
openness = bfi[, c("O1","O2", "O3", "O4", "O5")]
openness$O2 = 7 - openness$O2
openness$O5 = 7 - openness$O5
covariance_mat = cov(openness, use = "na.or.complete")
covariance_mat
variance = var(rowSums(openness), na.rm = TRUE)
variance
sum(covariance_mat) == variance
alpha(openness, check.keys = TRUE)
covariance_mat = cov(openness, use = "pairwise.complete.obs")
covariance_mat
variance = var(rowSums(openness), na.rm = TRUE)
variance
sum(covariance_mat) == variance
openness = bfi[, c("O1","O2", "O3", "O4", "O5")]
openness$O2 = 7 - openness$O2
openness$O5 = 7 - openness$O5
covariance_mat = cov(openness, use = "pairwise.complete.obs")
covariance_mat
variance = var(rowSums(openness), na.rm = TRUE)
variance
sum(covariance_mat) == variance
variance
covariance_mat
sum(covariance_mat)
variance
sum(covariance_mat)
variance
# 1. Bibliotheken laden
# install.packages("tidyverse")
library(tidyverse)
# 2. Daten einlesen
# Wir lesen die CSV ein.
raw_data <- read_csv("results-survey478754.csv", show_col_types = FALSE)
#Reinladen der Umfrage ins global Enviorment
survey.data <- read.csv("results-survey_cleaned.csv")
setwd("C:/Users/Annika/Documents/GitHub/Erhebungstechnik/Grafiken")
#Reinladen der Umfrage ins global Enviorment
survey.data <- read.csv("results-survey_cleaned.csv")
ggplot(raw_plot_data,
aes(x = reorder(Material, Zufriedenheit_Score, FUN = mean),
y = Zufriedenheit_Score)) +
# Zeigt die Verteilung (Box)
geom_boxplot(outlier.shape = NA, fill = "lightblue", alpha = 0.5) +
# Zeigt die echten Datenpunkte
geom_jitter(width = 0.2, height = 0.1, alpha = 0.3, color = "darkblue") +
# Durschnittspunkt (rot)
stat_summary(fun = mean, geom = "point", shape = 18, size = 4, color = "red") +
coord_flip() +
labs(title = "Wie einig sind sich die Studierenden?",
subtitle = "Boxplot zeigt Verteilung, roter Punkt ist der Durchschnitt",
x = "",
y = "Zufriedenheitsscore") +
theme_minimal() +
########################################
#Frage 3 - Welche Materialien werden als am effizientesten/besten
#für die Prüfungsvorbereitung empfunden´
#Dazu werden wir einen "Erfolgsscore" erstellen wir addieren die Werte zu den Themen
#Effizienz, Motivation, Sicherheit und Zeitaufwand und kategorisieren die Leute nach
#Erfolg und schauen dann welche Gruppen welche Materialien am Häufigsten Nutzen
survey.data <- survey.data %>%
mutate(
#Drehen des Zeitaufwand Scores damit er richtig  eingerechnet wird:
Effizienz_Score = 6 - Effekt_Zeitaufwand_Num,
#Jetzt einfach alle Zusammenaddieren(Höchste PunktZahl ist 20)
Erfolgs_Score = Effekt_Sicherheit_Num + Effekt_Motivation_Num + Effizienz_Score + Effekt_Relevanz_Num
)
library(dplyr)
ggplot(raw_plot_data,
aes(x = reorder(Material, Zufriedenheit_Score, FUN = mean),
y = Zufriedenheit_Score)) +
# Zeigt die Verteilung (Box)
geom_boxplot(outlier.shape = NA, fill = "lightblue", alpha = 0.5) +
# Zeigt die echten Datenpunkte
geom_jitter(width = 0.2, height = 0.1, alpha = 0.3, color = "darkblue") +
# Durschnittspunkt (rot)
stat_summary(fun = mean, geom = "point", shape = 18, size = 4, color = "red") +
coord_flip() +
labs(title = "Wie einig sind sich die Studierenden?",
subtitle = "Boxplot zeigt Verteilung, roter Punkt ist der Durchschnitt",
x = "",
y = "Zufriedenheitsscore") +
theme_minimal() +
########################################
#Frage 3 - Welche Materialien werden als am effizientesten/besten
#für die Prüfungsvorbereitung empfunden´
#Dazu werden wir einen "Erfolgsscore" erstellen wir addieren die Werte zu den Themen
#Effizienz, Motivation, Sicherheit und Zeitaufwand und kategorisieren die Leute nach
#Erfolg und schauen dann welche Gruppen welche Materialien am Häufigsten Nutzen
survey.data <- survey.data %>%
mutate(
#Drehen des Zeitaufwand Scores damit er richtig  eingerechnet wird:
Effizienz_Score = 6 - Effekt_Zeitaufwand_Num,
#Jetzt einfach alle Zusammenaddieren(Höchste PunktZahl ist 20)
Erfolgs_Score = Effekt_Sicherheit_Num + Effekt_Motivation_Num + Effizienz_Score + Effekt_Relevanz_Num
)
##
items_effekt <- survey.data %>%
select(
Effekt_Sicherheit_Num,
Effekt_Motivation_Num,
Effizienz_Score,
Effekt_Relevanz_Num
)
#Aufbereiten der Daten
material.data <- survey.data %>%
#Aus dem Datensatz den zufriendenheitsscore und alle Lernmaterialen extrahieren
select(Zufriedenheit_Score, starts_with("Nutzung_"))  %>%
#Verschiebung der Daten damit andere Funktionen später besser Funktionieren
pivot_longer(
cols = starts_with("Nutzung_"),
names_to = "Material",
values_to = "Nutzungshaeufigkeit"
)
#      )
# alpha(items_strukturiert, check.keys = TRUE)
#
## Reliability analysis
## Call: alpha(x = items_strukturiert, check.keys = TRUE)
##
## raw_alpha
## 0.5
## --> vielleicht lieber die Items einzeln betrachten?
##
items_effekt <- survey.data %>%
select(
Effekt_Sicherheit_Num,
Effekt_Motivation_Num,
Effizienz_Score,
Effekt_Relevanz_Num
)
#      )
# alpha(items_strukturiert, check.keys = TRUE)
#
## Reliability analysis
## Call: alpha(x = items_strukturiert, check.keys = TRUE)
##
## raw_alpha
## 0.5
## --> vielleicht lieber die Items einzeln betrachten?
##
items_effekt <- survey.data %>%
select(
Effekt_Sicherheit_Num,
Effekt_Motivation_Num,
6 - Effekt_Zeitaufwand_Num,
Effekt_Relevanz_Num
)
View(survey.data)
## Kommentar: Die Items korrelieren nicht wirklich miteinander (Cronbachs alpha <0.7)
items_strukturiert <- survey.data %>%
select(
Einstellung_Pflicht_Ball_Num,
Einstellung_Aufschieben_Num
)
alpha(items_strukturiert, check.keys = TRUE)
#Library installieren
library(tidyverse)
## Reliability analysis
## Call: alpha(x = items_strukturiert, check.keys = TRUE)
##
## raw_alpha
## 0.5
## --> vielleicht lieber die Items einzeln betrachten?
##
items_effekt <- survey.data %>%
select(
Effekt_Sicherheit_Num,
Effekt_Motivation_Num,
6 - Effekt_Zeitaufwand_Num,
Effekt_Relevanz_Num
)
#Library installieren
library(tidyverse)
library(psych)
## Reliability analysis
## Call: alpha(x = items_strukturiert, check.keys = TRUE)
##
## raw_alpha
## 0.5
## --> vielleicht lieber die Items einzeln betrachten?
##
items_effekt <- survey.data %>%
select(
Effekt_Sicherheit_Num,
Effekt_Motivation_Num,
6 - Effekt_Zeitaufwand_Num,
Effekt_Relevanz_Num
)
## Reliability analysis
## Call: alpha(x = items_strukturiert, check.keys = TRUE)
##
## raw_alpha
## 0.5
## --> vielleicht lieber die Items einzeln betrachten?
##
items_effekt <- survey.data %>%
mutate(Effizienz_Score = 6 - Effekt_Zeitaufwand_Num) %>%
select(
Effekt_Sicherheit_Num,
Effekt_Motivation_Num,
Effizienz_Score,
Effekt_Relevanz_Num
)
alpha(items_effekt, check.keys = TRUE)
